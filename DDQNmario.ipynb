{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDQNmario.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxCk8t74kSNk",
        "outputId": "0116ef00-f7dd-438e-f961-52b9ef70b4f9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install gym-super-mario-bros==7.3.0\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-super-mario-bros==7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b8/07460212c2568f78b02995834e7bdc25349e586473919e2983e01b984abf/gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 28.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 33.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40kB 23.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 27.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81kB 20.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92kB 18.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 122kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 133kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 184kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 194kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 19.0MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/5e/d652644d718454947b9e26d8a145eb7d1eff0f014dceee7bd88e4894b3f3/nes_py-8.1.6.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Collecting tqdm>=4.48.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.6-cp37-cp37m-linux_x86_64.whl size=436728 sha256=5cd0eae7be6db108afb1017c79596300553898bd7494f03c453f4b98939661e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/56/af/b84114d31ea6301a5c4651fb048bd6072646596a6ceb3bbc24\n",
            "Successfully built nes-py\n",
            "Installing collected packages: tqdm, nes-py, gym-super-mario-bros\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed gym-super-mario-bros-7.3.0 nes-py-8.1.6 tqdm-4.60.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWlH7WKHQCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18fe082-a9dd-446c-c80a-b723e19fbbc5"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "!apt update\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 43.1 kB/88.7\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [53.1 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [396 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
            "Hit:18 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [742 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,757 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,182 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [426 kB]\n",
            "Get:27 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [899 kB]\n",
            "Fetched 12.9 MB in 3s (4,948 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (1,280 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUF7SxFYkg2Q"
      },
      "source": [
        "# Environment Init :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSsYhAmLkkQg",
        "outputId": "0c8b4f2b-7293-42b7-d3a0-8a57c7f44d2e"
      },
      "source": [
        "# Initialize Super Mario environment\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp12KWhlk-wj"
      },
      "source": [
        "# Environment Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkKADPPekwzx"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import random, datetime, numpy as np\n",
        "from skimage import transform\n",
        "\n",
        "from gym.spaces import Box\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        resize_obs = transform.resize(observation, self.shape)\n",
        "        # cast float back to uint8\n",
        "        resize_obs *= 255\n",
        "        resize_obs = resize_obs.astype(np.uint8)\n",
        "        return resize_obs\n",
        "\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWbibgExrR5j"
      },
      "source": [
        "# Agent Mario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3bmmkgcrUJo"
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device=\"cuda\")\n",
        "        if checkpoint:\n",
        "            self.load(checkpoint)\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    action_idx (int): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "            if self.use_cuda:\n",
        "                state = torch.tensor(state).cuda()\n",
        "            else:\n",
        "                state = torch.tensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GbyjegGskJG"
      },
      "source": [
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint):\n",
        "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            state = torch.tensor(state).cuda()\n",
        "            next_state = torch.tensor(next_state).cuda()\n",
        "            action = torch.tensor([action]).cuda()\n",
        "            reward = torch.tensor([reward]).cuda()\n",
        "            done = torch.tensor([done]).cuda()\n",
        "        else:\n",
        "            state = torch.tensor(state)\n",
        "            next_state = torch.tensor(next_state)\n",
        "            action = torch.tensor([action])\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQNDoR0sm5L"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint):\n",
        "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI-PZatKsqnu"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint):\n",
        "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "        \n",
        "    def load(self, load_path):\n",
        "        if not load_path.exists():\n",
        "            raise ValueError(f\"{load_path} does not exist\")\n",
        "\n",
        "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
        "        exploration_rate = ckp.get('exploration_rate')\n",
        "        state_dict = ckp.get('model')\n",
        "\n",
        "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
        "        self.net.load_state_dict(state_dict)\n",
        "        self.exploration_rate = exploration_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6juYKi5suJG"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VG8DRkAswTD"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint):\n",
        "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-pxFAhKsR8F"
      },
      "source": [
        "# Mario Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQD4MSaysSau"
      },
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini cnn structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        # if h != 84:\n",
        "        #     raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        # if w != 84:\n",
        "        #     raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        input = input.float()\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSE92CT1s0VO"
      },
      "source": [
        "#Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyC76JKqs1Zj"
      },
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miRjpnz8xdRG"
      },
      "source": [
        "#Let's Play !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3B0-pJmOBTN",
        "outputId": "1b8766ed-e574-4c06-ed50-992335280ea4"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1000, 1000))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f51eef9dad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "RUpup82Vw5gB",
        "outputId": "1cbb8adc-c4a0-45ee-c57d-e0f26bbb84d2"
      },
      "source": [
        "import random, datetime\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "!pip install imageio\n",
        "import imageio\n",
        "\n",
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "#env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-2-v0')\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-1-4-v0')\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-2-1-v0')\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-1-3-v0')\n",
        "\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        ")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = TransformObservation(env, f=lambda x: x / 255.)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "\n",
        "checkpoint = Path('/content/trained_mario.chkpt')\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
        "mario.exploration_rate = 0.00 #mario.exploration_rate_min\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 20\n",
        "\n",
        "# result = cv2.VideoWriter('./mario_video.avi',  \n",
        "#                          cv2.VideoWriter_fourcc(*'MJPG'), \n",
        "#                          episodes, (256,256))\n",
        "\n",
        "liste_video_tot = []\n",
        "liste_steps = []\n",
        "liste_rewards = []\n",
        "for e in range(episodes):\n",
        "    print(f\"épisode {e}\")\n",
        "    liste_video_ind = []\n",
        "\n",
        "    state = env.reset()\n",
        "    i=0\n",
        "    R = 0\n",
        "    while True:\n",
        "      with torch.no_grad():\n",
        "        i+=1\n",
        "        screen = env.render(mode='rgb_array')\n",
        "        screen2 = np.copy(screen)\n",
        "\n",
        "        # result.write(screen2)\n",
        "\n",
        "        # plt.imshow(screen)\n",
        "        liste_video_ind.append(screen2)\n",
        "        # ipythondisplay.clear_output(wait=True)\n",
        "        # ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        action = mario.act(state)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        R+=reward\n",
        "        if info['flag_get'] :\n",
        "          R+=10000\n",
        "\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        logger.log_step(reward, None, None)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done or info['flag_get']:\n",
        "            liste_steps.append(i)\n",
        "            liste_rewards.append(R)\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "    liste_video_tot.append(liste_video_ind)\n",
        "\n",
        "    if e % 1 == 0:\n",
        "        logger.record(\n",
        "            episode=e,\n",
        "            epsilon=mario.exploration_rate,\n",
        "            step=mario.curr_step\n",
        "        )\n",
        "\n",
        "\n",
        "liste_len = np.array([len(i) for i in liste_video_ind])\n",
        "argmax = np.argmax(liste_rewards)\n",
        "argmax2 = np.argmax(liste_len)\n",
        "print(liste_rewards, \"\\n\")\n",
        "print(argmax)\n",
        "imageio.mimsave('mario.gif', [np.array(img) for img in liste_video_tot[argmax]], fps=13)\n",
        "# imageio.mimsave('mario2.gif', [np.array(img) for img in liste_video_tot[argmax2]], fps=13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n",
            "Loading model at /content/trained_mario.chkpt with exploration rate 0.1\n",
            "épisode 0\n",
            "Episode 0 - Step 24 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.508 - Time 2021-04-30T15:41:47\n",
            "épisode 1\n",
            "Episode 1 - Step 48 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.855 - Time 2021-04-30T15:41:48\n",
            "épisode 2\n",
            "Episode 2 - Step 72 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.847 - Time 2021-04-30T15:41:49\n",
            "épisode 3\n",
            "Episode 3 - Step 96 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.862 - Time 2021-04-30T15:41:50\n",
            "épisode 4\n",
            "Episode 4 - Step 120 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.848 - Time 2021-04-30T15:41:51\n",
            "épisode 5\n",
            "Episode 5 - Step 144 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.845 - Time 2021-04-30T15:41:51\n",
            "épisode 6\n",
            "Episode 6 - Step 168 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.83 - Time 2021-04-30T15:41:52\n",
            "épisode 7\n",
            "Episode 7 - Step 192 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.839 - Time 2021-04-30T15:41:53\n",
            "épisode 8\n",
            "Episode 8 - Step 216 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.83 - Time 2021-04-30T15:41:54\n",
            "épisode 9\n",
            "Episode 9 - Step 240 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.945 - Time 2021-04-30T15:41:55\n",
            "épisode 10\n",
            "Episode 10 - Step 264 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.82 - Time 2021-04-30T15:41:56\n",
            "épisode 11\n",
            "Episode 11 - Step 288 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.825 - Time 2021-04-30T15:41:57\n",
            "épisode 12\n",
            "Episode 12 - Step 312 - Epsilon 0.1 - Mean Reward 120.0 - Mean Length 24.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.829 - Time 2021-04-30T15:41:57\n",
            "épisode 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 13 - Step 468 - Epsilon 0.1 - Mean Reward 168.286 - Mean Length 33.429 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 3.512 - Time 2021-04-30T15:42:01\n",
            "épisode 14\n",
            "Episode 14 - Step 492 - Epsilon 0.1 - Mean Reward 165.067 - Mean Length 32.8 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.829 - Time 2021-04-30T15:42:02\n",
            "épisode 15\n",
            "Episode 15 - Step 516 - Epsilon 0.1 - Mean Reward 162.25 - Mean Length 32.25 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.843 - Time 2021-04-30T15:42:03\n",
            "épisode 16\n",
            "Episode 16 - Step 540 - Epsilon 0.1 - Mean Reward 159.765 - Mean Length 31.765 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.816 - Time 2021-04-30T15:42:03\n",
            "épisode 17\n",
            "Episode 17 - Step 564 - Epsilon 0.1 - Mean Reward 157.556 - Mean Length 31.333 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.832 - Time 2021-04-30T15:42:04\n",
            "épisode 18\n",
            "Episode 18 - Step 751 - Epsilon 0.1 - Mean Reward 190.474 - Mean Length 39.526 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.059 - Time 2021-04-30T15:42:08\n",
            "épisode 19\n",
            "Episode 19 - Step 775 - Epsilon 0.1 - Mean Reward 186.95 - Mean Length 38.75 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.917 - Time 2021-04-30T15:42:09\n",
            "[120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 796.0, 120.0, 120.0, 120.0, 120.0, 783.0, 120.0] \n",
            "\n",
            "13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}